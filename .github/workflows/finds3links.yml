name: FindS3Links

on:
  workflow_dispatch:
    inputs:
      domain:
        description: "DomÃ­nio base (ex.: example.com) â€” sem http/https"
        required: true
        default: "example.com"
      max_pages:
        description: "MÃ¡x. pÃ¡ginas por host (crawl controlado)"
        required: false
        default: "10"
      timeout_sec:
        description: "Timeout (s) por requisiÃ§Ã£o HTTP"
        required: false
        default: "20"

permissions:
  contents: read

jobs:
  discover:
    runs-on: ubuntu-latest
    outputs:
      s3_url: ${{ steps.test.outputs.s3_url }}
    steps:
      - name: Preparar ambiente
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq || true
          mkdir -p artifacts
          : > artifacts/seed_urls.txt
          : > artifacts/root_urls.txt
          : > artifacts/subdomains.txt
          : > artifacts/visited.txt
          : > artifacts/queue.txt
          : > artifacts/s3_candidates.txt
          : > artifacts/s3_found.txt
          echo "[]" > artifacts/results.json

      - name: Validar formato do domÃ­nio
        id: validate
        shell: bash
        env:
          DOMAIN: ${{ inputs.domain }}
        run: |
          set -euo pipefail
          if ! [[ "$DOMAIN" =~ ^([a-z0-9]([-a-z0-9]*[a-z0-9])?\.)+[a-z]{2,63}$ ]]; then
            echo "âŒ DomÃ­nio invÃ¡lido: '$DOMAIN'. Use algo como 'example.com'."
            exit 1
          fi
          echo "âœ… DomÃ­nio aceito: $DOMAIN"

      - name: Resolver DNS e checar alcance HTTP
        id: dnscheck
        shell: bash
        env:
          DOMAIN: ${{ inputs.domain }}
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          echo "ðŸ” Resolvendo DNS para: $DOMAIN"
          if ! getent hosts "$DOMAIN" >/dev/null 2>&1; then
            echo "âŒ DNS nÃ£o resolve para '$DOMAIN'."
            exit 1
          fi
          echo "âœ… DNS OK: $(getent hosts "$DOMAIN" | awk '{print $1}' | paste -sd, -)"

          echo "ðŸŒ Testando HTTPS/HTTP (HEAD curto)"
          ok=0
          for scheme in https http; do
            if curl -fsSI --max-time "${TOUT:-20}" "$scheme://$DOMAIN/" >/dev/null 2>&1; then
              echo "âœ… $scheme OK"
              ok=1
              break
            else
              echo "âš ï¸ $scheme falhou"
            fi
          done
          [ "$ok" -eq 1 ] || { echo "âŒ Sem alcance HTTP/HTTPS. Encerrando."; exit 1; }

      - name: Coletar subdomÃ­nios (passivo) a partir do domÃ­nio
        id: subdomains
        shell: bash
        env:
          DOMAIN: ${{ inputs.domain }}
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          UA="FindS3Links/secure-1.1 (+github-actions)"
          SEED="artifacts/seed_urls.txt"
          SUBS="artifacts/subdomains.txt"
          ROOT_HTML="artifacts/root.html"

          echo "https://$DOMAIN/" > "$SEED"
          echo "http://$DOMAIN/" >> "$SEED"

          # Baixa a pÃ¡gina raiz (https preferencial)
          curl -fsSL -A "$UA" --max-time "${TOUT:-20}" "https://$DOMAIN/" -o "$ROOT_HTML" || \
          curl -fsSL -A "$UA" --max-time "${TOUT:-20}" "http://$DOMAIN/" -o "$ROOT_HTML" || true

          # Extrai URLs absolutas referenciadas
          grep -Eoi 'https?://[^"'\'' )]+' "$ROOT_HTML" | sort -u > artifacts/root_urls.txt || true

          # Extrai subdomÃ­nios que terminam em .DOMAIN (lowercase); corrige Ã¢ncora $ no awk
          awk -F/ '{print $3}' artifacts/root_urls.txt \
            | awk -v d=".$DOMAIN" 'tolower($0) ~ tolower(d)"$" {print tolower($0)}' \
            | grep -Ev "^${DOMAIN}$" \
            | sort -u > "$SUBS" || true

          # Fallback leve de subdomÃ­nios comuns (mantÃ©m passivo e seguro)
          for prefix in www app api cdn assets static; do
            echo "${prefix}.${DOMAIN}"
          done | sort -u >> "$SUBS" || true
          sort -u -o "$SUBS" "$SUBS"

          echo "ðŸ”Ž SubdomÃ­nios (passivo + fallback leve):"
          if [ -s "$SUBS" ]; then sed -n '1,200p' "$SUBS"; else echo "(nenhum)"; fi

      - name: Crawl domÃ­nio e subdomÃ­nios (mesmo host) e extrair S3
        id: crawl
        shell: bash
        env:
          DOMAIN: ${{ inputs.domain }}
          MAXP: ${{ inputs.max_pages }}
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          UA="FindS3Links/secure-1.1 (+github-actions)"
          VIS="artifacts/visited.txt"
          QUE="artifacts/queue.txt"
          CAND="artifacts/s3_candidates.txt"
          MAX="${MAXP:-10}"
          T="${TOUT:-20}"

          # seeds: domÃ­nio principal
          echo "https://$DOMAIN/" > "$QUE"
          echo "http://$DOMAIN/" >> "$QUE"

          # seeds: subdomÃ­nios
          if [ -s artifacts/subdomains.txt ]; then
            while IFS= read -r h; do
              [ -z "$h" ] && continue
              echo "https://$h/" >> "$QUE"
              echo "http://$h/" >> "$QUE"
            done < artifacts/subdomains.txt
          fi

          pages=0
          while IFS= read -r url || [ -n "${url:-}" ]; do
            [ -z "$url" ] && continue
            # dedup visita
            if grep -qxF "$url" "$VIS" 2>/dev/null; then continue; fi
            echo "$url" >> "$VIS"

            tmp="artifacts/$(echo "$url" | tr '/:' '__').html"
            curl -fsSL -A "$UA" --max-time "$T" "$url" -o "$tmp" || continue

            # Extrai links absolutos
            grep -Eoi 'https?://[^"'\'' )]+' "$tmp" | sort -u > "$tmp.links" || true

            # Enfileira somente links do MESMO HOST (ignora binÃ¡rios)
            host=$(printf '%s' "$url" | awk -F/ '{print $3}')
            awk -F/ -v h="$host" '$3==h {print $0}' "$tmp.links" \
              | sed -E 's/[#?].*$//' \
              | grep -Ev '\.(png|jpe?g|gif|svg|ico|css|pdf|zip|gz|tgz|bz2|7z|mp4|mp3)$' \
              | while read -r same; do
                  grep -qxF "$same" "$VIS" 2>/dev/null || grep -qxF "$same" "$QUE" 2>/dev/null || echo "$same" >> "$QUE"
                done

            # Extrai possÃ­veis links S3 da pÃ¡gina (inclui s3-website e dualstack)
            grep -Eoi '(https?://[^"'\'' ]*\.s3[^"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true
            grep -Eoi '(https?://s3[^/"'\'' ]*\.amazonaws\.com/[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true
            grep -Eoi '(https?://[^/"'\'' ]*\.s3-website[^/"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true
            grep -Eoi '(https?://[^/"'\'' ]*\.s3\.dualstack\.[^/"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true

            # Baixa e inspeciona JS do mesmo host apenas para achar S3 (nÃ£o segue JS no crawl)
            awk -F/ -v h="$host" '$3==h {print $0}' "$tmp.links" \
              | grep -E '\.js($|\?)' \
              | sed -E 's/[#].*$//' \
              | while read -r jsu; do
                  jstmp="artifacts/$(echo "$jsu" | tr '/:' '__').js"
                  curl -fsSL -A "$UA" --max-time "$T" "$jsu" -o "$jstmp" || continue
                  grep -Eoi '(https?://[^"'\'' ]*\.s3[^"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$jstmp" | sort -u >> "$CAND" || true
                  grep -Eoi '(https?://s3[^/"'\'' ]*\.amazonaws\.com/[^"'\'' ]*)' "$jstmp" | sort -u >> "$CAND" || true
                  grep -Eoi '(https?://[^/"'\'' ]*\.s3-website[^/"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$jstmp" | sort -u >> "$CAND" || true
                done

            pages=$((pages+1))
            [ "$pages" -ge "$MAX" ] && break
          done < <(cat "$QUE")

          # Normaliza candidatos e dedup
          if [ -s "$CAND" ]; then
            sed -E 's/[#?].*$//' "$CAND" \
              | sed -E 's#://+#://#' \
              | sed -E 's#/{2,}#/#g' \
              | sort -u > "${CAND}.norm"
            mv "${CAND}.norm" "$CAND"
          fi

          echo "ðŸ“„ PÃ¡ginas visitadas: $pages"
          echo "ðŸ§© SubdomÃ­nios salvos em artifacts/subdomains.txt"
          echo "ðŸª£ Candidatos S3 salvos em artifacts/s3_candidates.txt"

      - name: Testar links S3 (paralelo) e resumir
        id: test
        shell: bash
        env:
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          CAND="artifacts/s3_candidates.txt"
          RES="artifacts/results.json"
          FOUND="artifacts/s3_found.txt"
          T="${TOUT:-20}"
          : > "$FOUND"

          probe_one() {
            link="$1"
            code="$(curl -s -I -L --max-time "$T" "$link" | awk '/^HTTP/{print $2}' | tail -n1)"
            if [ -z "$code" ] || [ "$code" = "000" ]; then
              if curl -s -L --max-time "$T" -o /tmp/_s3.bin "$link"; then
                [ -s /tmp/_s3.bin ] && code="200"
              fi
            fi
            jq -n --arg url "$link" --arg code "${code:-}" '{url:$url, http_code:$code}'
          }

          if [ -s "$CAND" ]; then
            mapfile -t JSONS < <(cat "$CAND" | xargs -I{} -P 8 bash -c 'probe_one "$@"' _ {})
            printf '%s\n' "${JSONS[@]}" | jq -s '.' > "$RES"
            jq -r '.[] | select(.http_code=="200") | .url' "$RES" | head -n1 > "$FOUND" || true
          else
            echo "[]" > "$RES"
          fi

          echo
          echo "========= S3 Discovery Summary ========="
          if [ -s "$FOUND" ]; then
            echo "âœ… Bucket pÃºblico encontrado:"
            cat "$FOUND"
            echo "s3_url=$(cat "$FOUND")" >> "$GITHUB_OUTPUT"
          else
            echo "âŒ Nenhum bucket pÃºblico encontrado."
            echo "s3_url=" >> "$GITHUB_OUTPUT"
          fi
          echo "----------------------------------------"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: finds3links-results
          path: artifacts/
