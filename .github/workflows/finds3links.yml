name: Find S3 Links

on:
  workflow_dispatch:
    inputs:
      subdomain:
        description: "Subdomínio de partida (ex.: app.example.com)"
        required: true
        default: "app.example.com"
      max_pages:
        description: "Máximo de páginas a visitar (mesmo host)"
        required: false
        default: "12"
      timeout_sec:
        description: "Timeout (segundos) por requisição HTTP"
        required: false
        default: "8"

permissions:
  contents: read

jobs:
  find-s3:
    runs-on: ubuntu-latest
    outputs:
      s3_url: ${{ steps.test.outputs.s3_url }}
    steps:
      - name: Prep
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq || true
          mkdir -p artifacts
          : > artifacts/queue.txt
          : > artifacts/visited.txt
          : > artifacts/subdomains.txt
          : > artifacts/s3_candidates.txt
          echo "[]" > artifacts/results.json
          : > artifacts/s3_found.txt

      - name: Crawl (same host) + extrair S3
        id: crawl
        shell: bash
        env:
          SUBDOMAIN: ${{ inputs.subdomain }}
          MAXP: ${{ inputs.max_pages }}
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          UA="FindS3Links/1.2 (+github-actions)"
          HOST="$SUBDOMAIN"
          CAND="artifacts/s3_candidates.txt"
          VIS="artifacts/visited.txt"
          QUE="artifacts/queue.txt"
          MAX="$MAXP"
          T="$TOUT"

          # seeds http/https
          echo "https://$HOST/" > "$QUE"
          echo "http://$HOST/" >> "$QUE"

          pages=0
          while IFS= read -r url || [ -n "${url:-}" ]; do
            [ -z "$url" ] && continue
            # dedup visita
            if grep -qxF "$url" "$VIS" 2>/dev/null; then continue; fi
            echo "$url" >> "$VIS"

            # baixa página
            tmp="artifacts/$(echo "$url" | tr '/:' '__').html"
            curl -fsSL -A "$UA" --max-time "$T" "$url" -o "$tmp" || continue

            # extrai links absolutos
            grep -Eoi 'https?://[^"'\'' )]+' "$tmp" | sort -u > "$tmp.links" || true

            # empilha links do MESMO HOST (BFS simples)
            awk -F/ -v h="$HOST" '$3==h {print $0}' "$tmp.links" \
              | sed -E 's/[#?].*$//' \
              | grep -Ev '\.(png|jpe?g|gif|svg|ico|css|js|pdf|zip|gz|tgz|bz2|7z|mp4|mp3)$' \
              | while read -r same; do
                  grep -qxF "$same" "$VIS" 2>/dev/null || grep -qxF "$same" "$QUE" 2>/dev/null || echo "$same" >> "$QUE"
                done

            # extrai possíveis links S3 dessa página
            grep -Eoi '(https?://[^"'\'' ]*\.s3[^"'\'' ]*\.amazonaws\.com[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true
            grep -Eoi '(https?://s3[^/"'\'' ]*\.amazonaws\.com/[^"'\'' ]*)' "$tmp" | sort -u >> "$CAND" || true

            pages=$((pages+1))
            [ "$pages" -ge "$MAX" ] && break
          done < <(cat "$QUE")

          # únicos
          sort -u -o "$CAND" "$CAND"

          echo "Subdomínio alvo: $HOST"
          echo "Páginas visitadas: $pages"
          echo "Links S3 candidatos:"
          sed -n '1,200p' "$CAND" || echo "(nenhum)"

      - name: Testar links S3 e resumir
        id: test
        shell: bash
        env:
          TOUT: ${{ inputs.timeout_sec }}
        run: |
          set -euo pipefail
          CAND="artifacts/s3_candidates.txt"
          RES="artifacts/results.json"
          FOUND="artifacts/s3_found.txt"
          T="$TOUT"

          RESULTS=()
          FOUND_URL=""

          if [ -s "$CAND" ]; then
            while IFS= read -r link; do
              [ -z "$link" ] && continue
              code=$(curl -s -I -L --max-time "$T" "$link" | awk '/^HTTP/{print $2}' | tail -n1)
              # fallback GET: se baixou e tem conteúdo, assume 200
              if [ -z "$code" ] || [ "$code" = "000" ]; then
                if curl -s -L --max-time "$T" -o /tmp/_s3.bin "$link"; then
                  [ -s /tmp/_s3.bin ] && code="200"
                fi
              fi
              entry=$(jq -n --arg url "$link" --arg code "${code:-}" '{url:$url, http_code:$code}')
              RESULTS+=("$entry")
              if [ "$code" = "200" ] && [ -z "$FOUND_URL" ]; then
                FOUND_URL="$link"
              fi
            done < "$CAND"
          fi

          printf '%s\n' "${RESULTS[@]:-}" | jq -s '.' > "$RES"

          echo
          echo "========= S3 Discovery Summary ========="
          if [ -n "$FOUND_URL" ]; then
            echo "✅ Bucket público encontrado:"
            echo "$FOUND_URL" | tee "$FOUND"
            echo "s3_url=$FOUND_URL" >> "$GITHUB_OUTPUT"
          else
            echo "❌ Nenhum bucket público encontrado."
            echo "s3_url=" >> "$GITHUB_OUTPUT"
          fi
          echo "----------------------------------------"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: s3-scan-results
          path: artifacts/
